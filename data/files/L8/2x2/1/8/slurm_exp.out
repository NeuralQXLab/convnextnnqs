Job running on nodes: cholesky-gpu02
Configuring sharding...
Number of distributed processes = 1
Number of total devices: 1
0/1 : global [CudaDevice(id=0)]
0/1 : local [CudaDevice(id=0)]
0/1 : hostname:  cholesky-gpu02
/mnt/beegfs/workdir/rajah.nutakki/uv_envs/nk_pro_update/.venv/lib/python3.12/site-packages/netket/vqs/mc/mc_state/state.py:260: UserWarning: For performance reasons, we suggest to use a power-of-two chunk size.
  self.chunk_size = chunk_size
Computing expectation values...
Using net_type=Vanilla
FlipExpSum(
    # attributes
    module = SymmExpSum(
        # attributes
        module = ConvNext(
            # attributes
            lattice_shape = (8, 8)
            n_blocks = (8,)
            features = (72,)
            expansion_factor = 2
            Head = OutputHead
            kernel_size = (2, 2)
            downsample_factor = 2
            final_features = 72
            extract_patches = extract_patches_as2d
        )
        symm_group = PermutationGroup(elems=[Id(), Refl(45째)O[-1/2,1/2], Rot(90째), Glide[0,-1]O[-1/2,0], Rot(180째), Glide[1,-1], Rot(-90째), Glide[1,0]O[0,1/2]], degree=64)
        character_id = None
    )
)
vstate.n_chains = 32
vstates.n_samples = 320000
vstate.n_discard_per_chain = 10000
Finished computing expectation values
Time taken 9540.699137210846s
