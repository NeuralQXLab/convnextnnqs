Job running on nodes: cholesky-gpu01
Configuring sharding...
Number of distributed processes = 1
Number of total devices: 1
0/1 : global [CudaDevice(id=0)]
0/1 : local [CudaDevice(id=0)]
0/1 : hostname:  cholesky-gpu01
/mnt/beegfs/workdir/rajah.nutakki/uv_envs/nk_pro_update/.venv/lib/python3.12/site-packages/netket/vqs/mc/mc_state/state.py:260: UserWarning: For performance reasons, we suggest to use a power-of-two chunk size.
  self.chunk_size = chunk_size
Computing expectation values...
Using net_type= NoPatching
FlipExpSum(
    # attributes
    module = SymmExpSum(
        # attributes
        module = ConvNext_nopatching(
            # attributes
            init_kernel_size = (6, 6)
            n_blocks = (3,)
            features = (72,)
            expansion_factor = 2
            kernel_size = (3, 3)
            unitcell_shape = (2, 2)
            lattice_shape = (6, 6)
            final_features = 72
            reshape_function = reshape_xy
        )
        symm_group = PermutationGroup(elems=[Id(), Refl(45째)O[-1/2,1/2], Rot(90째), Glide[0,-1]O[-1/2,0], Rot(180째), Glide[1,-1], Rot(-90째), Glide[1,0]O[0,1/2]], degree=36)
        character_id = None
    )
)
vstate.n_chains = 32
vstates.n_samples = 320000
vstate.n_discard_per_chain = 10000
Finished computing expectation values
Time taken 5993.876988172531s
