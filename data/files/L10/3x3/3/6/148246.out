Job running on nodes: jean-zay-iam25
Loading cpuarch/amd
  WARNING: Using "module load cpuarch/amd" is deprecated,
    use "module load arch/a100" instead.
recursion  started at Wed Oct 16 11:43:19 PM CEST 2024
python -u -O /lustre/fswork/projects/rech/iqu/uvm91ap/repos/netket_pro/deepnets/optimization/run.py --L=10 --J 0.8 1.0 --n_blocks 4 --features 72 --expansion_factor=2 --downsample_factor=2 --kernel_width=3 --output_head=Vanilla --samples_per_rank=512 --chains_per_rank=512 --discard_fraction=0.0 --iters 2500 2500 2500 2500 --lr 0.01 0.01 0.01 0.01 --alpha 0.5 0.5 0.5 0.5 --diag_shift 0.01 0.01 0.01 0.01 --diag_shift_end 0.0001 0.0001 0.0001 0.0001 --r=1e-06 --chunk_size=256 --save_every=100 --symmetries=0 --symmetry_ramping=1 --momentum=0.9 --post_iters=50 --double_precision=1 --time_it=0 --show_progress=0 --checkpoint=1 --seed=226  --save_base /gpfswork/rech/iqu/uvm91ap/projects/deepNQS/ConvNext/03_10_24/L=6/symm_ramp/6/ >> /gpfswork/rech/iqu/uvm91ap/projects/deepNQS/ConvNext/03_10_24/L=6/symm_ramp/6/${SLURM_JOB_ID}.out &
Configuring sharding...
Number of distributed processes = 1
Number of total devices: 8
0/1 : global [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3), CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7)]
0/1 : local [CudaDevice(id=0), CudaDevice(id=1), CudaDevice(id=2), CudaDevice(id=3), CudaDevice(id=4), CudaDevice(id=5), CudaDevice(id=6), CudaDevice(id=7)]
0/1 : hostname:  jean-zay-iam25
2024-10-16 23:43:37.611493: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.
Double precision enabled =  True
chunk_size = 256
Running optimization...
Symmetry stage 0 on process 0:
Total number of model parameters = 99720
Using minSR
restoring checkpoint #2500
restoring checkpoint #2500
Symmetry stage 1 on process 0:
Total number of model parameters = 99720
Using minSR
restoring checkpoint #2500
Symmetry stage 2 on process 0:
Total number of model parameters = 99720
Using minSR
restoring checkpoint #2500
Symmetry stage 3 on process 0:
Total number of model parameters = 99720
Using minSR
restoring checkpoint #0
/var/spool/slurmd/job148246/slurm_script: line 26: [: : integer expression expected
Time limit reached, resubmitting th sbatch...
Submitted batch job 236589
